{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Different Decoding Strategies with Hugging Face Transformers\n",
    "In this notebook, we will explore various decoding strategies for text generation using a small encoder-decoder model from Hugging Face's Transformers library. We'll apply these strategies to two different datasets:\n",
    "\n",
    "- Translation task where deterministic strategies are expected to perform better.\n",
    "- Summarization task where stochastic strategies might yield more diverse and informative outputs.\n",
    "\n",
    "The decoding strategies we'll test include:\n",
    "\n",
    "1. Greedy Search\n",
    "2. Beam Search\n",
    "3. Temperature Sampling\n",
    "4. Top-k Sampling\n",
    "5. Top-p (Nucleus) Sampling\n",
    "\n",
    "We'll define a custom ```generate_text``` function to apply these strategies and evaluate their performance using appropriate metrics for each dataset.\n",
    "\n",
    "Here are some useful links you might want to check:\n",
    "- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- [transformers\\AutoModel](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)\n",
    "- [transformers\\AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [Google's T5](https://arxiv.org/pdf/1910.10683)\n",
    "- [T5-small](https://huggingface.co/google-t5/t5-small)\n",
    "- [Flan-T5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets sacrebleu rouge_score evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer\n",
    "\n",
    "We'll use the ```flan-T5-small``` model, which is a small encoder-decoder model suitable for both story_gen and summarization tasks. This model is based on Google's ```t5-small``` model, but fine-tuned on more than 1000 additional tasks covering also more languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jviserass/miniconda3/envs/sd/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, strategy='greedy', max_length=50, **kwargs):\n",
    "    \"\"\"Generates text based on the specified decoding strategy.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be processed by the model.\n",
    "        strategy (str, optional): The decoding strategy to use. Defaults to 'greedy'.\n",
    "            Options include:\n",
    "            - 'greedy': Greedy search decoding.\n",
    "            - 'beam': Beam search decoding.\n",
    "            - 'temperature': Temperature sampling.\n",
    "            - 'top-k': Top-k sampling.\n",
    "            - 'top-p': Top-p (nucleus) sampling.\n",
    "            - 'contrastive': Contrastive search decoding.\n",
    "        max_length (int, optional): The maximum length of the generated text. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments specific to the decoding strategy.\n",
    "\n",
    "    Keyword Args:\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n",
    "            Used when `strategy='beam'`.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            Used when `strategy='temperature'`.\n",
    "        top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k filtering. Defaults to 50.\n",
    "            Used when `strategy='top-k'` or when `strategy='contrastive'`.\n",
    "        top_p (float, optional): Cumulative probability for nucleus sampling. Defaults to 0.95.\n",
    "            Used when `strategy='top-p'`.\n",
    "        penalty_alpha (float, optional): Contrastive search penalty factor. Defaults to 0.6.\n",
    "            Used when `strategy='contrastive'`.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the decoding strategy.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown decoding strategy is specified.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'greedy':\n",
    "            output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        elif strategy == 'beam':\n",
    "            num_beams = kwargs.get('num_beams', 5)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True\n",
    "            )\n",
    "        elif strategy == 'temperature':\n",
    "            temperature = kwargs.get('temperature', 1.0)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, temperature=temperature\n",
    "            )\n",
    "        elif strategy == 'top-k':\n",
    "            top_k = kwargs.get('top_k', 50)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_k=top_k\n",
    "            )\n",
    "        elif strategy == 'top-p':\n",
    "            top_p = kwargs.get('top_p', 0.95)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_p=top_p\n",
    "            )\n",
    "        elif strategy == 'contrastive':\n",
    "            penalty_alpha = kwargs.get('penalty_alpha', 0.6)\n",
    "            top_k = kwargs.get('top_k', 4)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, penalty_alpha=penalty_alpha, top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy: {}\".format(strategy))\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Neural Machine Translation\n",
    "The WMT16 English-German dataset is a collection of parallel sentences in English and German used for machine translation tasks. It is part of the Conference on Machine story_gen (WMT) shared tasks, which are benchmarks for evaluating machine story_gen systems. The dataset contains professionally translated sentences and covers a variety of topics, making it ideal for training and evaluating story_gen models.\n",
    "\n",
    "We'll use a subset (1% of the test split) of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 4548885/4548885 [00:02<00:00, 1650860.99 examples/s]\n",
      "Generating validation split: 100%|██████████| 2169/2169 [00:00<00:00, 746732.77 examples/s]\n",
      "Generating test split: 100%|██████████| 2999/2999 [00:00<00:00, 929553.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_nmt = load_dataset('wmt16', 'de-en', split='test[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Lamb war es wichtig zu betonen, dass sein \"süßer Hund\" aber noch lebe und wahrscheinlich aufgeregt sei, und er sagte, die Familienkontakte der toten Frau könnten auf dem Handy gefunden werden.\n",
      "\n",
      "Target Text:\n",
      "Lamb made a point to say his \"sweet dog\" was there alive and probably upset, and said the dead woman's family contacts could be found on her phone.\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_nmt['translation'])\n",
    "input_text = sample['de']\n",
    "target_text = sample['en']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nTarget Text:\")\n",
    "print(target_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the dataset to prepare it for input into the T5 model. The T5 model expects input in a specific format, including a task prefix. This is because the T5 model is a multi-purpose model, being able to perform several task, so we need to tell it what it must do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the translation task.\n",
    "\n",
    "    Args:\n",
    "        examples list[dict]: A list of dictionaries containing 'de' and 'en' keys with English and German sentences.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of lists with added 'src_texts' and 'tgt_texts' keys for model input and target.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'translate German to English: ' to the German sentence.\n",
    "    - Stores the result in 'src_texts'.\n",
    "    - Copies the English sentence to 'tgt_texts'.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a \"translate German to English: \" prefix\n",
    "    texts['src_texts'] = ['translate German to English: ' + ex['de'] for ex in examples]\n",
    "    texts['tgt_texts'] = [ex['en'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_nmt_preproc = preprocess_translation(dataset_nmt[\"translation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can generate a translation using the t5 model. From a random dataset, we are going to create translations using the different implemented strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate German to English: Bei der Begegnung soll es aber auch um den Konflikt mit den Palästinensern und die diskutierte Zwei-Staaten-Lösung gehen.\n",
      "Original translation: \n",
      "The meeting was also planned to cover the conflict with the Palestinians and the disputed two state solution.\n",
      "Greedy Search: \n",
      "The onset of the conflict should also be a part of the conflict with the Palestinians and the debated two-seat solution.\n",
      "Beam Search: \n",
      "However, it will also be a conflict with the Palästinians and the debated two-staaten solution.\n",
      "Temperature Sampling: \n",
      "Although the initiative is an important step but also a step in the fight against a Palestinian and the debated two-staaten solution.\n",
      "Top-k Sampling: \n",
      "The conclusion should therefore make it possible for the conflict to proceed with the Palestinians and to appoint the two-step solution.\n",
      "Top-p Sampling: \n",
      "When it comes to the introduction, this may be a combination of mediation, social networking and debate.\n",
      "Contrastive Search: \n",
      "The decision should also be taken on the conflict with the Palestinians and the debated two-seat solution.\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_nmt_preproc['src_texts']))\n",
    "random_src_sentence = dataset_nmt_preproc['src_texts'][random_index]\n",
    "random_tgt_sentence = dataset_nmt_preproc['tgt_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "translation_greedy= generate_text(random_src_sentence, strategy='greedy')\n",
    "translation_beam_search = generate_text(random_src_sentence, strategy='beam', num_beams=5)\n",
    "translation_temperature = generate_text(random_src_sentence, strategy='temperature', temperature=0.7)\n",
    "translation_top_k = generate_text(random_src_sentence, strategy='top-k', top_k=50)\n",
    "translation_top_p = generate_text(random_src_sentence, strategy='top-p', top_p=0.95)\n",
    "translation_contrastive = generate_text(random_src_sentence, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Original translation: \\n{random_tgt_sentence}\")\n",
    "print(f\"Greedy Search: \\n{translation_greedy}\")\n",
    "print(f\"Beam Search: \\n{translation_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{translation_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{translation_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{translation_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{translation_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see something different between the deterministic and the stochastic strategies? Try different random sentences.\n",
    "Las estrategias deterministas intentan traducciones mas literales, que las hacen a veces menos naturales o con errores de traduccion directos. Por otro lado las estocásticas generan traducciones mas variadas aportando alternativas distintas y a veces menos precisas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric: BLEU Score\n",
    "#### What is BLEU Score?\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text that has been machine-translated from one language to another. It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of words).\n",
    "\n",
    "BLEU-4: Considers up to 4-gram matches between the candidate and reference translations. It provides a balance between precision (matching words) and fluency (maintaining the structure of the language). Using BLEU-4 allows us to capture not just individual word matches (unigrams) but also phrases of up to four words. This makes the evaluation more sensitive to the quality of the translation in terms of both accuracy and fluency.\n",
    "\n",
    "We'll use the ```sacrebleu``` implementation for a standardized BLEU score calculation (which is BLEU-4). You can check the details [here](https://aclanthology.org/W14-3346.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 8.15kB [00:00, 13.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategy: greedy\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for beam: 21.86\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for beam: 21.86\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for temperature: 9.55\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for temperature: 9.55\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for top-k: 9.46\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-k: 9.46\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-p: 12.80\n",
      "Evaluating strategy: contrastive\n",
      "BLEU-4 score for top-p: 12.80\n",
      "Evaluating strategy: contrastive\n",
      "BLEU-4 score for contrastive: 19.34\n",
      "BLEU-4 score for contrastive: 19.34\n"
     ]
    }
   ],
   "source": [
    "strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p', 'contrastive']\n",
    "results = {}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_beams': 8,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'penalty_alpha': 0.6\n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"Evaluating strategy: {strategy}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]): \n",
    "        pred = generate_text(src_text, strategy=strategy, **hyperparameters)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])  # SacreBLEU expects a list of references\n",
    "\n",
    "    # Compute BLEU-4 score\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    results[strategy] = bleu['score']\n",
    "    print(f\"BLEU-4 score for {strategy}: {bleu['score']:.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando num_beams=1\n",
      "num_beams=1, BLEU-4=20.99\n",
      "Evaluando num_beams=2\n",
      "num_beams=1, BLEU-4=20.99\n",
      "Evaluando num_beams=2\n",
      "num_beams=2, BLEU-4=21.56\n",
      "Evaluando num_beams=3\n",
      "num_beams=2, BLEU-4=21.56\n",
      "Evaluando num_beams=3\n",
      "num_beams=3, BLEU-4=21.94\n",
      "Evaluando num_beams=4\n",
      "num_beams=3, BLEU-4=21.94\n",
      "Evaluando num_beams=4\n",
      "num_beams=4, BLEU-4=21.04\n",
      "Evaluando num_beams=5\n",
      "num_beams=4, BLEU-4=21.04\n",
      "Evaluando num_beams=5\n",
      "num_beams=5, BLEU-4=21.14\n",
      "Evaluando num_beams=6\n",
      "num_beams=5, BLEU-4=21.14\n",
      "Evaluando num_beams=6\n",
      "num_beams=6, BLEU-4=20.80\n",
      "Evaluando num_beams=7\n",
      "num_beams=6, BLEU-4=20.80\n",
      "Evaluando num_beams=7\n",
      "num_beams=7, BLEU-4=21.17\n",
      "Evaluando num_beams=8\n",
      "num_beams=7, BLEU-4=21.17\n",
      "Evaluando num_beams=8\n",
      "num_beams=8, BLEU-4=21.86\n",
      "Evaluando num_beams=9\n",
      "num_beams=8, BLEU-4=21.86\n",
      "Evaluando num_beams=9\n",
      "num_beams=9, BLEU-4=22.53\n",
      "Evaluando num_beams=10\n",
      "num_beams=9, BLEU-4=22.53\n",
      "Evaluando num_beams=10\n",
      "num_beams=10, BLEU-4=21.65\n",
      "\n",
      "El mejor num_beams es 9 con BLEU-4=22.53\n",
      "\n",
      "Resultados de BLEU-4 para cada num_beams:\n",
      "num_beams=1: BLEU-4=20.99\n",
      "num_beams=2: BLEU-4=21.56\n",
      "num_beams=3: BLEU-4=21.94\n",
      "num_beams=4: BLEU-4=21.04\n",
      "num_beams=5: BLEU-4=21.14\n",
      "num_beams=6: BLEU-4=20.80\n",
      "num_beams=7: BLEU-4=21.17\n",
      "num_beams=8: BLEU-4=21.86\n",
      "num_beams=9: BLEU-4=22.53\n",
      "num_beams=10: BLEU-4=21.65\n",
      "num_beams=10, BLEU-4=21.65\n",
      "\n",
      "El mejor num_beams es 9 con BLEU-4=22.53\n",
      "\n",
      "Resultados de BLEU-4 para cada num_beams:\n",
      "num_beams=1: BLEU-4=20.99\n",
      "num_beams=2: BLEU-4=21.56\n",
      "num_beams=3: BLEU-4=21.94\n",
      "num_beams=4: BLEU-4=21.04\n",
      "num_beams=5: BLEU-4=21.14\n",
      "num_beams=6: BLEU-4=20.80\n",
      "num_beams=7: BLEU-4=21.17\n",
      "num_beams=8: BLEU-4=21.86\n",
      "num_beams=9: BLEU-4=22.53\n",
      "num_beams=10: BLEU-4=21.65\n"
     ]
    }
   ],
   "source": [
    "# Búsqueda del mejor num_beams para Beam Search\n",
    "best_bleu = 0\n",
    "best_num_beams = 1\n",
    "bleu_scores = []\n",
    "\n",
    "for num_beams in range(1, 11):\n",
    "    print(f\"Evaluando num_beams={num_beams}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]):\n",
    "        pred = generate_text(src_text, strategy='beam', num_beams=num_beams)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    bleu_score = bleu['score']\n",
    "    bleu_scores.append((num_beams, bleu_score))\n",
    "    print(f\"num_beams={num_beams}, BLEU-4={bleu_score:.2f}\")\n",
    "    if bleu_score > best_bleu:\n",
    "        best_bleu = bleu_score\n",
    "        best_num_beams = num_beams\n",
    "\n",
    "print(f\"\\nEl mejor num_beams es {best_num_beams} con BLEU-4={best_bleu:.2f}\")\n",
    "# Si quieres ver todos los resultados:\n",
    "print(\"\\nResultados de BLEU-4 para cada num_beams:\")\n",
    "for nb, score in bleu_scores:\n",
    "    print(f\"num_beams={nb}: BLEU-4={score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above translations and the BLEU score of the different strategies, which strategy would you choose for this use case?\n",
    "\n",
    "Para el caso de traducción automática, elegiría la estrategia Beam Search. Es la que obtiene el mayor BLEU-4 (21.14) y produce traducciones más coherentes y precisas que las estrategias estocásticas ya que las estrategias estocásticas generan más variedad, pero sacrifican precisión y calidad en este contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Story generation\n",
    "\n",
    "The ```WritingPrompts``` dataset is a collection of imaginative prompts and corresponding stories from the Reddit community. It contains over 300,000 stories written in response to various prompts, making it suitable for training and evaluating models on creative text generation tasks.\n",
    "\n",
    "We'll use a subset of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 232360/232360 [00:00<00:00, 2287863.05 examples/s]\n",
      "Generating sample_length_10_to_292 split: 100%|██████████| 1000/1000 [00:00<00:00, 1072712.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_st_gen = load_dataset('llm-aes/writing-prompts', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      " Science has found the key to immortality , but there 's a catch : it can only be administered at birth . You are a member of the last mortal generation .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_st_gen['prompt'])\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to tell the t5 model to generate text after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_story_generation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the story generation task.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): A list of dictionaries containing 'prompt' key.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of list with added 'src_texts' for model input.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'Write a story based on: ' to the prompt.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a task prefix\n",
    "    texts['src_texts'] = ['Write a story based on: ' + ex['prompt'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_st_gen_preproc = preprocess_story_generation(dataset_st_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no original story to compare how good the model generates a story, you should compare the different decoding strategies by looking at some random stories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story based on:  The floor is lava .\n",
      "\n",
      "Greedy Search: \n",
      "The floor is lava. It is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a lava-like substance that is a\n",
      "Beam Search: \n",
      "The floor is lava.\n",
      "Temperature Sampling: \n",
      "a young baby boy's crawling through the floor is a lava-like creature. The lava is a type of lava that is swollen with water. The lava is a big boulder that is too big.\n",
      "Top-k Sampling: \n",
      "If the floor is lava, it could be the next thing. Someone made up his mind when we were not so we went down to the lava rock and a lot of lava boiled inside. The lava was not the evaporation that we might see today, but rather our lava.\n",
      "Top-p Sampling: \n",
      "The house that the smoldering owner built has recently been transformed into a nightmare of what could be a disaster and the fact that the building is now being renovated. Now one of the first settlers of the first century had a floor filled with lava, the basement floor has been raised and the house is now completely covered with lava.\n",
      "Contrastive Search: \n",
      "The floor is lava. It is a lava-like substance that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is a liquid that is \n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_st_gen_preproc['src_texts']))\n",
    "random_src_sentence = dataset_st_gen_preproc['src_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "story_gen_greedy= generate_text(random_src_sentence, max_length=300, strategy='greedy')\n",
    "story_gen_beam_search = generate_text(random_src_sentence, max_length=300, strategy='beam', num_beams=5)\n",
    "story_gen_temperature = generate_text(random_src_sentence, max_length=300, strategy='temperature', temperature=0.7)\n",
    "story_gen_top_k = generate_text(random_src_sentence, max_length=300, strategy='top-k', top_k=50)\n",
    "story_gen_top_p = generate_text(random_src_sentence, max_length=300, strategy='top-p', top_p=0.92)\n",
    "story_gen_contrastive = generate_text(random_src_sentence, max_length=300, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Greedy Search: \\n{story_gen_greedy}\")\n",
    "print(f\"Beam Search: \\n{story_gen_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{story_gen_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{story_gen_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{story_gen_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{story_gen_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above generated stories of the different strategies, which strategy would you choose for this use case?\n",
    "\n",
    "Para la generación de historias, las estrategias estocásticas como Top-p, Top-k o Temperature Sampling parecen ser mejores. Aunque pueden producir resultados menos coherentes, nos permiten generar textos más creativos y variados. En este caso, las estrategias deterministas tienden a repetir frases o quedarse atascadas, mientras que Top-p y Top-k ofrecen historias más originales y menos repetitivas. Por tanto, elegiría Top-p o Top-k para este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis\n",
    "\n",
    "Try different hyperparameter values for the decoding strategies, try to optimize the BLEU score for the Neural Machine Translation case and generate better stories in the second use case.\n",
    "\n",
    "- Which optimal configuration have you found for use case 1? Which are your conclusions based on your analysis?\n",
    "\n",
    "Para el caso 1 , la mejor configuración fue num_beams = 9 con BLEU-4=22.53. Aunque se podria usar num_beans = 4 ya que solo disminuye el BLEU por 0.5 y disminuiria el coste computacional. Las estrategias estocásticas y el ajuste de temperature, top-k o top-p no superaron a Beam Search en BLEU. Por tanto Beam Search es la mejor opción para traducción precisa y estable.\n",
    "\n",
    "- Which optimal configuration have you found for use case 2? Which are your conclusions bsaed on your analysis?\n",
    "\n",
    "Para el caso 2 (generación de historias), los mejores resultados en creatividad y coherencia se obtuvieron con Top-p Sampling usando top_p=0.92 y max_length=300. Aunque un top_p entre 0.9 y 0.95 da historias variadas pero comprensibles. Top-k también funciona bien con valores entre 40 y 60. Para tareas creativas, Top-p o Top-k con valores intermedios generan textos más interesantes y menos repetitivos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
